{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement Learning fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foreword <a class=\"tocSkip\">\n",
    "    \n",
    "How this course works (pedagogy):\n",
    "- one notebook to rule them all (them = the concepts)\n",
    "- no slides\n",
    "- short exercices along the way\n",
    "- a bit of live coding\n",
    "- two class breaks for you to breathe\n",
    "    \n",
    "What you should expect:\n",
    "- some plain words notions,\n",
    "- but also a fair bit of (hopefully painless) rigorous notations and concepts.\n",
    "- Also most things will be fully written down to increase your autonomy in replaying the notebook.\n",
    "\n",
    "Color code:\n",
    "<div class=\"alert alert-success\">Key results in green boxes</div>\n",
    "<div class=\"alert alert-warning\">Exercices in yellow boxes</div>\n",
    "\n",
    "Prerequisites:\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Prerequisites:**\n",
    "- Basic algebra\n",
    "- Random variables, probability distributions.\n",
    "    \n",
    "**Useful but not compulsory:**\n",
    "- Random processes, Markov chains.\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1><span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Class-goals\" data-toc-modified-id=\"Class-goals-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Class goals</a></span></li><li><span><a href=\"#Ruining-the-suspense-with-a-general-definition-(5-minutes)\" data-toc-modified-id=\"Ruining-the-suspense-with-a-general-definition-(5-minutes)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Ruining the suspense with a general definition (5 minutes)</a></span></li><li><span><a href=\"#RL-within-Machine-Learning-(5-minutes)\" data-toc-modified-id=\"RL-within-Machine-Learning-(5-minutes)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>RL within Machine Learning (5 minutes)</a></span></li><li><span><a href=\"#From-plain-words-to-variables-(5-minutes)\" data-toc-modified-id=\"From-plain-words-to-variables-(5-minutes)-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>From plain words to variables (5 minutes)</a></span></li><li><span><a href=\"#Modeling-sequential-decision-problems-with-Markov-Decision-Processes-(30-minutes)\" data-toc-modified-id=\"Modeling-sequential-decision-problems-with-Markov-Decision-Processes-(30-minutes)-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Modeling sequential decision problems with Markov Decision Processes (30 minutes)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Definition\" data-toc-modified-id=\"Definition-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Definition</a></span></li><li><span><a href=\"#Value-of-a-trajectory-/-of-a-policy\" data-toc-modified-id=\"Value-of-a-trajectory-/-of-a-policy-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Value of a trajectory / of a policy</a></span></li><li><span><a href=\"#Optimal-policies\" data-toc-modified-id=\"Optimal-policies-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Optimal policies</a></span></li><li><span><a href=\"#Stationary-distribution\" data-toc-modified-id=\"Stationary-distribution-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Stationary distribution</a></span></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Summary</a></span></li></ul></li><li><span><a href=\"#Characterizing-value-functions:-the-Bellman-equations-(20-minutes)\" data-toc-modified-id=\"Characterizing-value-functions:-the-Bellman-equations-(20-minutes)-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Characterizing value functions: the Bellman equations (20 minutes)</a></span></li><li><span><a href=\"#Dynamic-Programming-for-MDPs-(30-minutes)\" data-toc-modified-id=\"Dynamic-Programming-for-MDPs-(30-minutes)-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Dynamic Programming for MDPs (30 minutes)</a></span></li><li><span><a href=\"#Learning-optimal-value-functions-(30-minutes)\" data-toc-modified-id=\"Learning-optimal-value-functions-(30-minutes)-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Learning optimal value functions (30 minutes)</a></span></li><li><span><a href=\"#Direct-policy-optimization-(15-minutes)\" data-toc-modified-id=\"Direct-policy-optimization-(15-minutes)-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Direct policy optimization (15 minutes)</a></span></li><li><span><a href=\"#Three-fundamental-challenges-in-RL-(10-minutes)\" data-toc-modified-id=\"Three-fundamental-challenges-in-RL-(10-minutes)-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Three fundamental challenges in RL (10 minutes)</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Class goals\n",
    "\n",
    "- acquire the fundamental building blocks of RL:\n",
    "    - plain word notions\n",
    "    - MDPs, policies, optimality equations, etc.\n",
    "    - common notations\n",
    "    - key algorithms\n",
    "    - common misconceptions\n",
    "- key challenges in RL and their connection to RLVS lectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ruining the suspense with a general definition (5 minutes)\n",
    "\n",
    "\n",
    "What is Reinforcement Learning about?\n",
    "\n",
    "It is about learning to control dynamic systems.\n",
    "<img src=\"img/dynamic.png\" style=\"width: 400px;\"></img>\n",
    "Dynamic systems? **dynamic** evolution of $s$ and $o$ under $\\pi$.\n",
    "\n",
    "Our object of study:<br>\n",
    "We want to find a control policy $\\pi$ (with $u = \\pi(o)$) such that the system $\\Sigma$ behaves as we desire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Examples of RL problems <a class=\"tocSkip\">\n",
    "\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <td><img src=\"img/spiral.jpg\" style=\"width: 200px;\"></td>\n",
    "  <td style=\"border-right:1px solid;\">Exiting a spiral</td>\n",
    "  <td><img src=\"img/tests.jpg\" style=\"width: 200px;\"></td>\n",
    "  <td>Dynamic treatment regimes for HIV patients</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td><img src=\"img/pend.png\" style=\"width: 200px;\"></td>\n",
    "  <td style=\"border-right:1px solid;\">Cart-pole balancing</td>\n",
    "  <td><img src=\"img/waiting.jpg\" style=\"width: 200px;\"></td>\n",
    "  <td>Queueing problems</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td><img src=\"img/market.jpg\" style=\"width: 200px;\"></td>\n",
    "  <td style=\"border-right:1px solid;\">Portfolio management</td>\n",
    "  <td><img src=\"img/dam.jpg\" style=\"width: 200px;\"></td>\n",
    "  <td>Hydroelectric production</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "But also:\n",
    "- Elevator scheduling\n",
    "- Bicyle riding\n",
    "- Ship steering\n",
    "- Bioreactor control\n",
    "- Aerobatics helicopter control\n",
    "- Airport departures scheduling\n",
    "- Airlines scheduling\n",
    "- Robocup soccer\n",
    "- Video game playing (Quake, CS, Starcraft...)\n",
    "- Game of Go\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "Reinforcement Learning is about learning an optimal sequential behavior in a given environment.\n",
    "</div>\n",
    "\n",
    "Let's break this down.\n",
    "- sequential behavior in a given environment\n",
    "- optimal\n",
    "- learning\n",
    "\n",
    "<center><img src=\"img/dynamic.png\" style=\"width: 400px;\"></img></center>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Keywords:**\n",
    "- system to control / environment\n",
    "- control policy\n",
    "- optimality\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Warm-up poll:** \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL within Machine Learning (5 minutes)\n",
    "\n",
    "You may have had classes on Machine Learning before. There are three strongly distinct categories of problems in ML:\n",
    "- Supervised Learning\n",
    "- Unsupervised Learning\n",
    "- Reinforcement Learning\n",
    "\n",
    "Let's try to answer the following questions for each category.\n",
    "- What's the abstract problem we are trying to solve?\n",
    "- What's the data provided to the algorithms?\n",
    "- Give examples of algorithms in SL/UL/RL.  \n",
    "\n",
    "<center>\n",
    "<table border=\"1\">\n",
    "<tr>\n",
    "    <td> <b>Question</b> </td>\n",
    "    <td style=\"border-left: 1px solid black\"> <b>Supervised</b> </td>\n",
    "    <td style=\"border-left: 1px solid black\"> <b>Unsupervised</b> </td>\n",
    "    <td style=\"border-left: 1px solid black\"> <b>Reinforcement</b> </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> Target </td>\n",
    "    <td style=\"border-left: 1px solid black\"> $f(x)=y$ </td>\n",
    "    <td style=\"border-left: 1px solid black\"> $x\\in X$ </td>\n",
    "    <td style=\"border-left: 1px solid black\"> $\\pi(s)=a$ </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> Target (rephrased) </td>\n",
    "    <td style=\"border-left: 1px solid black\"> Predict outputs given inputs</td>\n",
    "    <td style=\"border-left: 1px solid black\"> Discover structure in data </td>\n",
    "    <td style=\"border-left: 1px solid black\"> Find an optimal behavior </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> Data </td>\n",
    "    <td style=\"border-left: 1px solid black\"> $\\left\\{\\left(x,y\\right)\\right\\}$ supervisor's labels </td>\n",
    "    <td style=\"border-left: 1px solid black\"> $\\left\\{x\\right\\}$ unlabelled data </td>\n",
    "    <td style=\"border-left: 1px solid black\"> $\\left\\{\\left(s,a,r,s'\\right)\\right\\}$ experience samples </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> Output </td>\n",
    "    <td style=\"border-left: 1px solid black\"> Classifier or regressor</td>\n",
    "    <td style=\"border-left: 1px solid black\"> Clusters or dimension reduction </td>\n",
    "    <td style=\"border-left: 1px solid black\"> Policies, value functions </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> Key algorithms </td>\n",
    "    <td style=\"border-left: 1px solid black\"> Neural networks, SVMs, etc.</td>\n",
    "    <td style=\"border-left: 1px solid black\"> k-means, PCA, etc. </td>\n",
    "    <td style=\"border-left: 1px solid black\"> Q-learning, Policy Gradients, etc. </td>\n",
    "</tr>\n",
    "</table>\n",
    "</center>\n",
    "\n",
    "This table helps distinguish the different natures of the problems tackled. The RL problem is about finding the optimal policy for a given environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is this different from Supervised Learning?\n",
    "- no correct $(s,a)$ example, rather $(s,a,r,s')$ samples\n",
    "- Delayed rewards, credit assignement, trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Poll:** How is RL different from SL?\n",
    "- a\n",
    "- b\n",
    "- c\n",
    "- d\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## From plain words to variables (5 minutes)\n",
    "\n",
    "### A medical prescription example <a class=\"tocSkip\">\n",
    "\n",
    "<img src=\"img/patient-doctor.png\" style=\"height: 200px;\">\n",
    "    \n",
    "A patient walks into a clinic with her medical file (medical history, x-rays, blood work, etc.). You, as her doctor, need to write a prescription. Let us use this example to formalize the process of deciding what to write on the prescription."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Patient variables <a class=\"tocSkip\">\n",
    "\n",
    "<center>\n",
    "<img src=\"img/patient_file.png\" style=\"height: 100px;\"> </img> <br>\n",
    "Patient state now: $S_0$  <br>\n",
    "Future states: $S_t$\n",
    "</center>\n",
    "\n",
    "The medical file of the patient allows us to define a number of variables that characterize the patient now. We will write $S_0$ the vector of these variables. Future measurements will be noted $S_t$.\n",
    "\n",
    "$S_t$ is a random vector, taking different values in a *patient description space* $S$ at different time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prescription <a class=\"tocSkip\">\n",
    "\n",
    "<center>\n",
    "<img src=\"img/prescription.png\" style=\"height: 100px;\"> </img> <br>\n",
    "Prescription: $\\left( A_t \\right)_{t\\in\\mathbb{N}} = (A_0, A_1, A_2, ...)$\n",
    "</center>\n",
    "\n",
    "The prescription is a series of recommendations we give to the patient over the course of treatment. It is thus a sequence $\\left( A_t \\right)_{t\\in\\mathbb{N}} = (A_0, A_1, A_2, ...)$ of variables $A_t$.\n",
    "\n",
    "These treatments $A_t$ are random variables too, taking their value in some space $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patient evolution <a class=\"tocSkip\">\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"img/patient_evolution.png\" style=\"height: 100px;\"> </img> <br>\n",
    "    $\\mathbb{P}(S_t)$?\n",
    "</center>\n",
    "\n",
    "The patient evolves over time steps. Her evolution follows a certain probability distribution $\\mathbb{P}(S_t)$ over descriptive states.\n",
    "\n",
    "So $\\left( S_t \\right)_{t\\in\\mathbb{N}}$ defines a *random process* that describes the patient's evolution under the influence of past $S_t$ and $A_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physician's goal <a class=\"tocSkip\">\n",
    "\n",
    "<img src=\"img/patient_happy.png\" style=\"height: 100px;\"> </img> <br>\n",
    "\n",
    "$$J \\left( \\left(S_t\\right)_{t\\in \\mathbb{N}}, \\left( A_t \\right)_{t\\in \\mathbb{N}} \\right)?$$\n",
    "\n",
    "The physician's goal is to bring the patient from an unhealthy state $S_0$ to a healthy situation.  \n",
    "\n",
    "This goal is not only defined by a final state of the patient but by the full trajectory followed by the variables $S_t$ and $A_t$. For example, prescribing a drug that damages the patient's liver, or letting the patient experience too much pain over the course of treatment is discouraged.\n",
    "\n",
    "We define a criterion $J \\left( \\left(S_t\\right)_{t\\in \\mathbb{N}}, \\left( A_t \\right)_{t\\in \\mathbb{N}} \\right)$ that allows to quantify how good a trajectory in the joint $S\\times A$ space is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap-up <a class=\"tocSkip\">\n",
    "\n",
    "- Patient state $S_t$  (random variable)\n",
    "- Physician instruction $A_t$ (random variable)\n",
    "- Prescription $\\left( A_t \\right)_{t\\in\\mathbb{N}}$   \n",
    "- Patient's evolution $\\mathbb{P}(S_t)$  \n",
    "- Patient's trajectory $\\left( S_t \\right)_{t\\in\\mathbb{N}}$ random process\n",
    "- Value of a trajectory $J \\left( \\left(S_t\\right)_{t\\in \\mathbb{N}}, \\left( A_t \\right)_{t\\in \\mathbb{N}} \\right)$  \n",
    "\n",
    "It seems reasonable that the physician's recommendation $\\mathbb{P}(A_t)$ at step $t$ be dependent on previously observed states $\\left(S_0, \\ldots, S_t\\right)$ and recommended treatments $\\left(A_0, \\ldots, A_{t-1}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common misconception <a class=\"tocSkip\">\n",
    "\n",
    "You will often see the following type of drawing, along with a sentence like \"RL is concerned with the problem on an agent performing actions to control an environment\". \n",
    "\n",
    "<img src=\"img/misconception.png\" style=\"height: 300px;\"></img>\n",
    "\n",
    "Although this sentence is not false *per se*, it conveys an important misconception that may be grounded in too simple anthropomorphic analogies. One often talks about the *state of the agent* or the *state of the environment*. The distinction here is confusing at best: there is no separation between agent and environment. A better vocabulary is to talk about a *system to control*, that is described through its observed *state*. This system is controlled by the application of actions issued from a *policy* or *control law*. The process of *learning* this policy is what RL is concerned with.\n",
    "\n",
    "Although less shiny, the drawing below may be less misleading.\n",
    "\n",
    "<img src=\"img/dynamic.png\" style=\"height: 300px;\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Three key notions <a class=\"tocSkip\">\n",
    "\n",
    "RL is a three-stage rocket answering the questions:  \n",
    "1. What is the system to control?  \n",
    "2. What is an optimal strategy?  \n",
    "3. How do we learn such a strategy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Poll:**\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling sequential decision problems with Markov Decision Processes (30 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition\n",
    "\n",
    "Let's take a higher view and develop a general theory for describing problems such as writing a prescription for our patient.\n",
    "\n",
    "Let us assume we have:\n",
    "- a set of states $S$ for the system to control,\n",
    "- a set of actions $A$ we can apply.\n",
    "\n",
    "Curing patients is a conceptually difficult task. \n",
    "To keep things grounded, we shall use a toy example called [FrozenLake](https://gym.openai.com/envs/FrozenLake-v0/) and work our way to more general concepts. It's also the occasion to familiarize with [OpenAI Gym](https://gym.openai.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import gym.envs.toy_text.frozen_lake as fl\n",
    "\n",
    "env = gym.make('FrozenLake-v0')\n",
    "_=env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at this problem's description (using for example `help(fl.FrozenLakeEnv)`). We read:\n",
    "\n",
    "`|  Winter is here. You and your friends were tossing around a frisbee at the park\n",
    "|  when you made a wild throw that left the frisbee out in the middle of the lake.\n",
    "|  The water is mostly frozen, but there are a few holes where the ice has melted.\n",
    "|  If you step into one of those holes, you'll fall into the freezing water.\n",
    "|  At this time, there's an international frisbee shortage, so it's absolutely imperative that\n",
    "|  you navigate across the lake and retrieve the disc.\n",
    "|  However, the ice is slippery, so you won't always move in the direction you intend.\n",
    "|  The surface is described using a grid like the following\n",
    "|  \n",
    "|      SFFF\n",
    "|      FHFH\n",
    "|      FFFH\n",
    "|      HFFG\n",
    "|  \n",
    "|  S : starting point, safe\n",
    "|  F : frozen surface, safe\n",
    "|  H : hole, fall to your doom\n",
    "|  G : goal, where the frisbee is located\n",
    "|  \n",
    "|  The episode ends when you reach the goal or fall in a hole.\n",
    "|  You receive a reward of 1 if you reach the goal, and zero otherwise.`\n",
    "\n",
    "So it's a game of navigation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Questions:</b><br>What are the possible states of an agent in this game?<br> What are its possible actions?<br>How would you describe the result of action $a$ in state $s$?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><a href=\"#answers1\" data-toggle=\"collapse\"><b>Answers:</b></a><br>\n",
    "<div id=\"answers1\" class=\"collapse\">\n",
    "States set: the 16 positions on the map.<br>\n",
    "Actions set: the 4 actions $\\{$N,S,E,W$\\}$<br>\n",
    "$s'$ resulting from $(s,a)$ follows a distribution $P(s'|s,a)$<br>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(16)\n",
      "Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At every time step, the system state is $S_t$ and we decide to apply action $A_t$. This results in observing a new state $S_{t+1}$ and receiving a scalar reward signal $R_t$ for this transition.\n",
    "\n",
    "$R_t$ tells us how happy we are with the last transition.\n",
    "\n",
    "Note that $S_t$, $A_t$, $S_{t+1}$ and $R_t$ are random variables.\n",
    "\n",
    "For example, in FrozenLake, all transitions have reward 0 except for the one that reaches the goal, which yields reward 1. Let's verify this and introduce a few utility functions on the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '←', 1: '↓', 2: '→', 3: '↑'}\n",
      "Apply → from (3, 2):\n",
      "  Reach ((3, 2)) and get reward 0.0 with proba 0.3333333333333333.\n",
      "  Reach ((3, 3)) and get reward 1.0 with proba 0.3333333333333333.\n",
      "  Reach ((2, 2)) and get reward 0.0 with proba 0.3333333333333333.\n"
     ]
    }
   ],
   "source": [
    "actions = {fl.LEFT: '\\u2190', fl.DOWN: '\\u2193', fl.RIGHT: '\\u2192', fl.UP: '\\u2191'}\n",
    "\n",
    "def to_s(row,col):\n",
    "    return row*env.unwrapped.ncol+col\n",
    "\n",
    "def to_row_col(s):\n",
    "    col = s%env.unwrapped.ncol\n",
    "    row = int((s-col)/env.unwrapped.ncol)\n",
    "    return row,col\n",
    "\n",
    "print(actions)\n",
    "row=3\n",
    "col=2\n",
    "a=2\n",
    "print(\"Apply \", actions[2], \" from (\", row, \", \", col, \"):\", sep='')\n",
    "for tr in env.unwrapped.P[to_s(row,col)][a]:\n",
    "    print(\"  Reach (\", to_row_col(tr[1]), \") and get reward \", tr[2], \" with proba \", tr[0], \".\", sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now make our main assumption about the systems we want to control.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Fundamental assumption (Markov property)**\n",
    "$$\\mathbb{P}(S_{t+1},R_t|S_t, A_t, S_{t-1}, A_{t-1}, \\ldots, S_0, A_0) = \\mathbb{P}(S_{t+1},R_t|S_t, A_t)$$\n",
    "</div>\n",
    "    \n",
    "Such a system will be called a Markov Decision Process (MDP).\n",
    "\n",
    "One generally separates the state dynamics and the rewards by:\n",
    "$$\\mathbb{P}(S_{t+1},R_t|S_t, A_t) = \\mathbb{P}(S_{t+1}|S_t, A_t)\\cdot \\mathbb{P}(R_t|S_t, A_t, S_{t+1})$$\n",
    "\n",
    "Which leads in turn to the general definition of an MDP:\n",
    "<div class=\"alert alert-success\"><b>Markov Decision Process (MDP)</b><br>\n",
    "A Markov Decision Process is given by:\n",
    "<ul>\n",
    "<li> A set of states $S$\n",
    "<li> A set of actions $A$\n",
    "<li> A (Markovian) transition model $\\mathbb{P}\\left(S_{t+1} | S_t, A_t \\right)$, noted $p(s'|s,a)$\n",
    "<li> A reward model $\\mathbb{P}\\left( R_t | S_t, A_t, S_{t+1} \\right)$, noted $r(s,a)$ or $r(s,a,s')$\n",
    "<li> A set of discrete decision epochs $T=\\{0,1,\\ldots,H\\}$\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "Most of the results presented here can be found in M. L. Puterman's classic book, [Markov Decision Processes: Discrete Stochastic Dynamic Programming](https://www.wiley.com/en-us/Markov+Decision+Processes%3A+Discrete+Stochastic+Dynamic+Programming-p-9781118625873).\n",
    "\n",
    "If $H\\rightarrow\\infty$ we have an infinite horizon control problem.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "Since we will only work with infinite horizon problems, we shall identify the MDP with the 4-tuple $\\langle S,A,p,r\\rangle$.\n",
    "</div>\n",
    "    \n",
    "So, in RL, we wish to control the trajectory of a system that, we suppose, behaves as a Markov Decision Process.\n",
    "\n",
    "<img src=\"img/dynamic.png\" style=\"height: 240px;\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value of a trajectory / of a policy\n",
    "\n",
    "Suppose an oracle decides on how to choose actions at each time step according to the probability distribution $\\mathbb{P}(A_t)=\\pi(A_t)$. This collection of probability distributions is the oracle's **policy**. Given a distribution on an initial state $S_0$, it fully conditions the trajectory $S_0, A_0, R_0, S_1, A_1, R_1, \\ldots$.\n",
    "\n",
    "In FrozenLake as in the patient's example, some trajectories are better than others. We shall introduce a criterion to compare trajectories. Intuitively, this criterion should reflect the idea that a good policy accumulates as much reward as possible along a trajectory.\n",
    "\n",
    "Let's compare the policy that always moves to the right and the policy that always moves down by summing the rewards obtained along trajectories and then averaging these rewards across trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value of 'right' policy: 0.03138 variance: 0.17434246642743126\n",
      "value of 'left' policy:  0.0 variance: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "nb_episodes = 100000\n",
    "horizon = 200\n",
    "\n",
    "Vright = np.zeros(nb_episodes)\n",
    "for i in range(nb_episodes):\n",
    "    env.reset()\n",
    "    for t in range(horizon):\n",
    "        next_state, r, done,_ = env.step(fl.RIGHT)\n",
    "        Vright[i] += r\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "Vleft  = np.zeros(nb_episodes)\n",
    "for i in range(nb_episodes):\n",
    "    env.reset()\n",
    "    for t in range(horizon):\n",
    "        next_state, r, done,_ = env.step(fl.LEFT)\n",
    "        Vleft[i] += r\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "print(\"est. value of 'right' policy:\", np.mean(Vright), \"variance:\", np.std(Vright))\n",
    "print(\"est. value of 'left'  policy:\", np.mean(Vleft),  \"variance:\", np.std(Vleft))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the general case, this sum of rewards on an infinite horizon might be unbounded. So let us introduce the **$\\gamma$-discounted sum of rewards** (from a starting state $s$, under policy $\\pi$) random variable:\n",
    "$$G^\\pi(s) = \\sum\\limits_{t = 0}^\\infty \\gamma^t R_t \\quad \\Bigg| \\quad \\begin{array}{l}S_0 = s,\\\\ A_t \\sim \\pi(S_t),\\\\ S_{t+1}\\sim p(\\cdot|S_t,A_t),\\\\R_t = r(S_t,A_t,S_{t+1}).\\end{array}$$\n",
    "\n",
    "With $\\gamma \\in (0,1)$, this infinite sum of rewards represents what we can gain in the long-term by applying the actions from $\\pi$, given that a reward obtained $t$ time steps in the future is discounted by $\\gamma^t$. For bounded reward models, since $\\gamma <1$, this sum is always finite.\n",
    "\n",
    "Then, given a starting state $s$, we can define the value of $s$ under policy $\\pi$:\n",
    "$$V^\\pi(s) = \\mathbb{E} \\left[ G^\\pi(s) \\right]$$\n",
    "\n",
    "This defines the value function $V^\\pi$ of policy $\\pi$:\n",
    "<div class=\"alert alert-success\"><b>Value function $V^\\pi$ of a policy $\\pi$ under a $\\gamma$-discounted criterion</b><br>\n",
    "$$V^\\pi : \\left\\{\\begin{array}{ccl}\n",
    "S & \\rightarrow & \\mathbb{R}\\\\\n",
    "s & \\mapsto & V^\\pi(s)=\\mathbb{E}\\left( \\sum\\limits_{t = 0}^\\infty \\gamma^t R_t \\bigg| S_0 = s, \\pi \\right)\\end{array}\\right. $$\n",
    "</div>\n",
    "\n",
    "\n",
    "And, given a distribution $\\rho_0$ on starting states, we can map $\\pi$ to the scalar value:\n",
    "$$J(\\pi) = \\mathbb{E}_{s \\sim \\rho_0} \\left[ V^\\pi(s) \\right]$$\n",
    "\n",
    "Note that this definition is quite arbitrary: instead of the expected (discounted) sum of rewards, we could have taken the average reward over all time steps, or some other (more or less exotic) comparison criterion between policies.\n",
    "\n",
    "Most of the RL literature uses this discounted criterion (in some cases with $\\gamma=1$), some uses the average reward criterion, and few works venture into more exotic criteria. Today, we will limit ourselves to the discounted criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal policies\n",
    "\n",
    "The fog clears up a bit: we can now compare policies given an initial state (or initial state distribution).  \n",
    "We can now define what an optimal policy is.  \n",
    "\n",
    "<div class=\"alert alert-success\"><b>Optimal policy $\\pi^*$</b><br>\n",
    "$\\pi^*$ is said to be optimal iff $\\pi^* \\in \\arg\\max\\limits_{\\pi} V^\\pi$.<br>\n",
    "<br>\n",
    "    \n",
    "A policy is optimal if it **dominates** over any other policy in every state:\n",
    "$$\\pi^* \\textrm{ is optimal}\\Leftrightarrow \\forall s\\in S, \\ \\forall \\pi, \\ V^{\\pi^*}(s) \\geq V^\\pi(s)$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that one could also define a somewhat weaker notion of optimality, stating that a policy $\\pi^*$ is optimal if:\n",
    "$$\\pi^* \\in \\arg\\max_{\\pi} J(\\pi).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now get to our first fundamental result. Fortunately for us...  \n",
    "\n",
    "<div class=\"alert alert-success\"><b>Optimal policy theorem</b><br>\n",
    "For $\\left\\{\\begin{array}{l}\n",
    "\\gamma\\textrm{-discounted criterion}\\\\\n",
    "\\textrm{infinite horizon}\n",
    "\\end{array}\\right.$, \n",
    "there always exists at least one optimal stationary, deterministic, Markovian policy.\n",
    "</div>\n",
    "\n",
    "Let's explain a little:\n",
    "- Markovian : $\\left\\{\\begin{array}{l}\n",
    "\\forall \\left(s_i,a_i\\right)\\in \\left(S\\times A\\right)^{t-1}\\\\\n",
    "\\forall \\left(s'_i,a'_i\\right)\\in \\left(S\\times A\\right)^{t-1}\n",
    "\\end{array}\\right., \\pi\\left(A_t|S_0, A_0, \\ldots, S_t\\right) = \\pi\\left(A_t|S'_0, A'_0, \\ldots, S_t\\right)$.  \n",
    "One writes $\\pi(A_t|S_t)$.\n",
    "- Stationary : $\\forall (t,t')\\in \\mathbb{N}^2, \\pi(A_t|S_t=s) = \\pi(A_{t'}|S_{t'}=s)$.\n",
    "- Deterministic : $\\pi(A_t|history) = \\left\\{\\begin{array}{l}\n",
    "1\\textrm{ for a single }a\\\\\n",
    "0\\textrm{ otherwise}\n",
    "\\end{array}\\right.$.\n",
    "\n",
    "So in simpler words, we know that among all possible optimal policies, at least one is a function $\\pi:S\\rightarrow A$.\n",
    "\n",
    "That helps a lot: we don't have to search for optimal policies in a complex family of history-dependent, stochastic, non-stationary policies; instead we can simply search for a function $\\pi(s)=a$ that maps states to actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationary distribution\n",
    "\n",
    "Let's consider an MDP and a certain policy $\\pi$. Let's initialize the MDP to a starting state $s_0$ drawn from a distribution $\\rho_0(s)$ and let's look at how the state evolves across time steps.\n",
    "\n",
    "Because the stochastic process of $S_t$ is a Markov chain (since $\\pi$ is fixed, the probability of reaching $S_{t+1}$ is only conditionned by $S_t$), in the long run, the distribution of states follows a stationary distribution $\\rho^\\pi(s|s_0)$.\n",
    "\n",
    "This distribution is not necessarily unique: it depends on $s_0$. When all states are represented with non-zero probability in this distribution, the corresponding Markov chain is said to be *ergodic*. This is an assumption that will often be made to simply future reasoning, even if it is false most of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise**  \n",
    "What can we say about the stationary distribution of the Markov chain corresponding to:\n",
    "- the patient with a chronic disease under a policy that fights off the disease?\n",
    "- the patient with a deadly disease under a policy that doesn't cure her?\n",
    "- the FrozenLake example with a fixed random policy?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><a href=\"#ergodic\" data-toggle=\"collapse\"><b>Answer:</b></a><br>\n",
    "<div id=\"ergodic\" class=\"collapse\">\n",
    "\n",
    "The patient with a chronic disease under a policy that fights off the disease will most likely live a rather long life (let's say infinite, for the sake of this example) and will explore states that are linked to the evolution of the disease. The states corresponding to non-recoverable situations however will not be visited.\n",
    "    \n",
    "The patient with a deadly disease and a bad treatment policy will likely die, sadly. On an infinite horizon, the stationary distribution only has probability mass on the states corresponding to death.\n",
    "    \n",
    "Similarly, the FrozenLake example has several terminal states, either by reaching the goal or by falling into a hole. It should be noted however that for such episodic environments, it is possible to define an alternate distribution $\\rho^\\pi(s|s_0)$ that describes the distribution of states before termination.\n",
    "    \n",
    "Finally, the Mad Hatter's casino under a fixed random policy is a very nice ergodic Markov chain: from any starting state there is a non-zero probability of reaching any state in a finite number of steps. No terminal states in wonderland!\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Let's wrap this whole section up. Our goal was to formally define the search for the best strategy for our game of FrozenLake and the medical prescription problem. This has led us to formalizing the general **discrete-time stochastic optimal control problem**:\n",
    "- Environment (discrete time, non-deterministic, non-linear, Markov) $\\leftrightarrow$ MDP.\n",
    "- Behaviour $\\leftrightarrow$ control policy $\\pi : s\\mapsto a$.\n",
    "- Policy evaluation criterion $\\leftrightarrow$ $\\gamma$-discounted criterion.\n",
    "- Goal $\\leftrightarrow$ Maximize value function $V^\\pi(s)$.\n",
    "\n",
    "So we have built the first stage of our three-stage rocket.  \n",
    "The question was \"What is the system to control?\" and our answer is \"The system to control is a Markov Decision Process $\\langle S, A, p, r \\rangle$ and we will control it with a policy $\\pi:s\\mapsto a$ in order to optimize $\\mathbb{E} \\left( \\sum_t \\gamma^t R_t\\right)$\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Poll** The limits of MDP modeling   \n",
    "Can these systems be modeled as MDPs?   \n",
    "- Playing a tennis video game based on a single video frame\n",
    "- Playing a tennis video game based on a full physical description of the ball and the players\n",
    "- The game of Poker\n",
    "- The collaborative game of [Hanabi](https://en.wikipedia.org/wiki/Hanabi_(card_game))\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Let's take a short break**\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Characterizing value functions: the Bellman equations (20 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q functions\n",
    "- evaluation equation\n",
    "- optimality equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Programming for MDPs (30 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- value iteration\n",
    "- policy iteration and modified policy iteration\n",
    "- approximate dynamic programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's take a short break**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning optimal value functions (30 minutes)\n",
    "\n",
    "- reminder on stochastic approximation and SGD\n",
    "- one step of SGD is an approximate resolution of an optimization problem\n",
    "- eval equation -> TD\n",
    "- AVI -> QL\n",
    "- API -> SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct policy optimization (15 minutes)\n",
    "\n",
    "- max V -> DPS and PG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three fundamental challenges in RL (10 minutes)\n",
    "\n",
    "Function approximation, exploration, optimality.\n",
    "\n",
    "These challenges are intrinsic to RL.  \n",
    "But there are countless others, that depend on the context, e.g.:\n",
    "- Hierarchical RL\n",
    "- Multi agent RL\n",
    "- Partially observable MDPs\n",
    "- Robust RL\n",
    "- Offline RL\n",
    "- Transfer in RL\n",
    "\n",
    "Connection to RLVS classes (map of RLVS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "12px",
    "width": "186px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "327px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
